{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "84116bb1-d64b-4de2-ae6d-a3a05122e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Theme - Compact Kiwi Visualization Setup\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns, warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from ipywidgets import interact, IntSlider, Dropdown, Checkbox, Button, Output\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "warnings.filterwarnings('ignore')\n",
    "AI_COLORS = {'primary':'#8FBC8F','gold':'#DAA520','crypto':'#FF6347','accent':'#6B8E23','highlight':'#ADFF2F','dark':'#2F4F2F','neutral':'#F0FFF0'}\n",
    "kiwi_palette = [AI_COLORS['primary'], AI_COLORS['gold'], AI_COLORS['crypto'], AI_COLORS['accent'], AI_COLORS['highlight']]\n",
    "plt.style.use('default')\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(kiwi_palette)\n",
    "plt.rcParams.update({'figure.facecolor':AI_COLORS['neutral'],'figure.figsize':[12,8],'axes.facecolor':'white','axes.edgecolor':AI_COLORS['dark'],'axes.grid':True,'axes.titlecolor':AI_COLORS['dark'],'axes.titleweight':'bold','axes.titlesize':16,'axes.labelsize':12,'grid.color':AI_COLORS['primary'],'grid.alpha':0.3,'font.size':11,'xtick.color':AI_COLORS['dark'],'ytick.color':AI_COLORS['dark'],'legend.fontsize':10,'legend.frameon':True,'legend.facecolor':'white','legend.edgecolor':AI_COLORS['primary'],'lines.linewidth':2,'savefig.dpi':300,'savefig.bbox':'tight'})\n",
    "sns.set_context(\"notebook\", font_scale=1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af472306-a415-48e9-b3fc-134401e492a1",
   "metadata": {},
   "source": [
    "# The Problem\n",
    "\n",
    "Knowing and defining the complex relationships of assets on the markets, in order to group them as shared or related risks. This could prevent putting too much risk into a sector, and encourage healthy portfolio diversification. It could also be used to monitor the relationship dynamics of the assets over time, and detect if companies begin shifting sectors or becoming too entangled in other assets in your portfolio in real time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0035fa-c231-4f8e-be55-ba15d773705d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427b1ed3-42ef-400f-84ec-e84fd5af9427",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "> **Real Financial Market Data**\n",
    "\n",
    "This is an **unlabeled dataset** of 225+ diverse financial assets. The goal is to discover natural groupings based on shared risk profiles - will the model find traditional sectors like Tech vs Energy, or uncover complex risk relationships we don't immediately understand?\n",
    "\n",
    "### Dataset Information:\n",
    "**Source**: Yahoo Finance (yfinance)  \n",
    "**Time Period**: 5 years of historical data  \n",
    "**Date Range**: 2021-11-10 to 2025-03-24\n",
    "\n",
    "### Asset Universe:\n",
    "| Category | Count | Examples |\n",
    "|----------|-------|----------|\n",
    "| **Large Cap Stocks** | 50 | AAPL, MSFT, GOOGL, NVDA, TSLA |\n",
    "| **Mid Cap Stocks** | 30 | PLTR, SNOW, COIN, ROKU, ABNB |\n",
    "| **Energy Stocks** | 20 | XOM, CVX, COP, SLB, EOG |\n",
    "| **Financial Stocks** | 20 | JPM, GS, BAC, BLK, AXP |\n",
    "| **Broad Market ETFs** | 20 | SPY, QQQ, IWM, VOO, VTI |\n",
    "| **Sector ETFs** | 20 | XLK, XLF, XLE, XLV, XLI |\n",
    "| **Thematic ETFs** | 20 | ARKK, ICLN, HACK, ROBO, JETS |\n",
    "| **Commodities** | 30 | Gold, Silver, Oil, Copper, Wheat |\n",
    "| **Cryptocurrencies** | 20 | BTC, ETH, SOL, ADA, DOGE |\n",
    "\n",
    "### Features Engineered:\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| `Mean_Return` | Annualized average return |\n",
    "| `Volatility` | Annualized standard deviation |\n",
    "| `Sharpe_Ratio` | Risk-adjusted return metric |\n",
    "| `Skewness` | Distribution asymmetry |\n",
    "| `Kurtosis` | Tail risk measure |\n",
    "| `Downside_Volatility` | Downside risk |\n",
    "| `Max_Drawdown` | Largest peak-to-trough decline |\n",
    "| `Percentile_5/95` | Extreme return thresholds |\n",
    "| `Rolling_Volatility` | 30-day rolling volatility average |\n",
    "| `Win_Rate` | Percentage of positive days |\n",
    "| `Gain_Loss_Ratio` | Average gain vs average loss |\n",
    "| `Correlation_SPY` | Correlation with market (SPY) |\n",
    "| `Beta_SPY` | Systematic risk vs market |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430f43da",
   "metadata": {},
   "source": [
    "# Data Aquistion & Scraping\n",
    "First lets define our universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7931c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#our diverse financial assets\n",
    "FINANCIAL_ASSETS = [\n",
    "    # Large Cap US Stocks (50)\n",
    "    (\"AAPL\", \"Stock\", \"Tech\"), (\"MSFT\", \"Stock\", \"Tech\"), (\"GOOGL\", \"Stock\", \"Tech\"),\n",
    "    (\"AMZN\", \"Stock\", \"Tech\"), (\"NVDA\", \"Stock\", \"Tech\"), (\"META\", \"Stock\", \"Tech\"),\n",
    "    (\"TSLA\", \"Stock\", \"Auto\"), (\"BRK-B\", \"Stock\", \"Financial\"), (\"V\", \"Stock\", \"Financial\"),\n",
    "    (\"JNJ\", \"Stock\", \"Healthcare\"), (\"WMT\", \"Stock\", \"Retail\"), (\"JPM\", \"Stock\", \"Financial\"),\n",
    "    (\"MA\", \"Stock\", \"Financial\"), (\"PG\", \"Stock\", \"Consumer\"), (\"UNH\", \"Stock\", \"Healthcare\"),\n",
    "    (\"HD\", \"Stock\", \"Retail\"), (\"DIS\", \"Stock\", \"Media\"), (\"BAC\", \"Stock\", \"Financial\"),\n",
    "    (\"ADBE\", \"Stock\", \"Tech\"), (\"CRM\", \"Stock\", \"Tech\"), (\"NFLX\", \"Stock\", \"Media\"),\n",
    "    (\"CMCSA\", \"Stock\", \"Telecom\"), (\"PEP\", \"Stock\", \"Consumer\"), (\"COST\", \"Stock\", \"Retail\"),\n",
    "    (\"CSCO\", \"Stock\", \"Tech\"), (\"AVGO\", \"Stock\", \"Tech\"), (\"INTC\", \"Stock\", \"Tech\"),\n",
    "    (\"AMD\", \"Stock\", \"Tech\"), (\"QCOM\", \"Stock\", \"Tech\"), (\"TXN\", \"Stock\", \"Tech\"),\n",
    "    (\"INTU\", \"Stock\", \"Tech\"), (\"AMAT\", \"Stock\", \"Tech\"), (\"BKNG\", \"Stock\", \"Travel\"),\n",
    "    (\"AMGN\", \"Stock\", \"Healthcare\"), (\"SBUX\", \"Stock\", \"Consumer\"), (\"GILD\", \"Stock\", \"Healthcare\"),\n",
    "    (\"MDLZ\", \"Stock\", \"Consumer\"), (\"ADP\", \"Stock\", \"Tech\"), (\"ISRG\", \"Stock\", \"Healthcare\"),\n",
    "    (\"LRCX\", \"Stock\", \"Tech\"), (\"ADI\", \"Stock\", \"Tech\"), (\"REGN\", \"Stock\", \"Healthcare\"),\n",
    "    (\"PYPL\", \"Stock\", \"Financial\"), (\"MU\", \"Stock\", \"Tech\"), (\"VRTX\", \"Stock\", \"Healthcare\"),\n",
    "    (\"MRNA\", \"Stock\", \"Healthcare\"), (\"NXPI\", \"Stock\", \"Tech\"), (\"KLAC\", \"Stock\", \"Tech\"),\n",
    "    (\"MELI\", \"Stock\", \"Tech\"), (\"ASML\", \"Stock\", \"Tech\"),\n",
    "    \n",
    "    # Mid Cap Stocks (30)\n",
    "    (\"ABNB\", \"Stock\", \"Travel\"), (\"COIN\", \"Stock\", \"Financial\"), (\"ROKU\", \"Stock\", \"Tech\"),\n",
    "    (\"SNAP\", \"Stock\", \"Tech\"), (\"SPOT\", \"Stock\", \"Media\"), (\"SQ\", \"Stock\", \"Financial\"),\n",
    "    (\"RBLX\", \"Stock\", \"Tech\"), (\"PLTR\", \"Stock\", \"Tech\"), (\"SNOW\", \"Stock\", \"Tech\"),\n",
    "    (\"NET\", \"Stock\", \"Tech\"), (\"CRWD\", \"Stock\", \"Tech\"), (\"ZS\", \"Stock\", \"Tech\"),\n",
    "    (\"DDOG\", \"Stock\", \"Tech\"), (\"MDB\", \"Stock\", \"Tech\"), (\"TEAM\", \"Stock\", \"Tech\"),\n",
    "    (\"ZM\", \"Stock\", \"Tech\"), (\"DOCU\", \"Stock\", \"Tech\"), (\"UBER\", \"Stock\", \"Tech\"),\n",
    "    (\"LYFT\", \"Stock\", \"Tech\"), (\"DASH\", \"Stock\", \"Tech\"), (\"PINS\", \"Stock\", \"Tech\"),\n",
    "    (\"ETSY\", \"Stock\", \"Retail\"), (\"W\", \"Stock\", \"Retail\"), (\"CHWY\", \"Stock\", \"Retail\"),\n",
    "    (\"PTON\", \"Stock\", \"Consumer\"), (\"RIVN\", \"Stock\", \"Auto\"), (\"LCID\", \"Stock\", \"Auto\"),\n",
    "    (\"F\", \"Stock\", \"Auto\"), (\"GM\", \"Stock\", \"Auto\"), (\"NIO\", \"Stock\", \"Auto\"),\n",
    "    \n",
    "    # Energy Stocks (20)\n",
    "    (\"XOM\", \"Stock\", \"Energy\"), (\"CVX\", \"Stock\", \"Energy\"), (\"COP\", \"Stock\", \"Energy\"),\n",
    "    (\"SLB\", \"Stock\", \"Energy\"), (\"EOG\", \"Stock\", \"Energy\"), (\"MPC\", \"Stock\", \"Energy\"),\n",
    "    (\"PSX\", \"Stock\", \"Energy\"), (\"VLO\", \"Stock\", \"Energy\"), (\"OXY\", \"Stock\", \"Energy\"),\n",
    "    (\"HAL\", \"Stock\", \"Energy\"), (\"BKR\", \"Stock\", \"Energy\"), (\"DVN\", \"Stock\", \"Energy\"),\n",
    "    (\"FANG\", \"Stock\", \"Energy\"), (\"HES\", \"Stock\", \"Energy\"), (\"MRO\", \"Stock\", \"Energy\"),\n",
    "    (\"APA\", \"Stock\", \"Energy\"), (\"CTRA\", \"Stock\", \"Energy\"), (\"OVV\", \"Stock\", \"Energy\"),\n",
    "    (\"NEE\", \"Stock\", \"Energy\"), (\"DUK\", \"Stock\", \"Energy\"),\n",
    "    \n",
    "    # Financial Stocks (20)\n",
    "    (\"GS\", \"Stock\", \"Financial\"), (\"MS\", \"Stock\", \"Financial\"), (\"C\", \"Stock\", \"Financial\"),\n",
    "    (\"WFC\", \"Stock\", \"Financial\"), (\"USB\", \"Stock\", \"Financial\"), (\"PNC\", \"Stock\", \"Financial\"),\n",
    "    (\"TFC\", \"Stock\", \"Financial\"), (\"SCHW\", \"Stock\", \"Financial\"), (\"BLK\", \"Stock\", \"Financial\"),\n",
    "    (\"AXP\", \"Stock\", \"Financial\"), (\"SPGI\", \"Stock\", \"Financial\"), (\"MCO\", \"Stock\", \"Financial\"),\n",
    "    (\"ICE\", \"Stock\", \"Financial\"), (\"CME\", \"Stock\", \"Financial\"), (\"BX\", \"Stock\", \"Financial\"),\n",
    "    (\"KKR\", \"Stock\", \"Financial\"), (\"APO\", \"Stock\", \"Financial\"), (\"COF\", \"Stock\", \"Financial\"),\n",
    "    (\"DFS\", \"Stock\", \"Financial\"), (\"SYF\", \"Stock\", \"Financial\"),\n",
    "    \n",
    "    # ETFs - Broad Market (20)\n",
    "    (\"SPY\", \"ETF\", \"Broad Market\"), (\"QQQ\", \"ETF\", \"Tech\"), (\"IWM\", \"ETF\", \"Small Cap\"),\n",
    "    (\"DIA\", \"ETF\", \"Blue Chip\"), (\"VTI\", \"ETF\", \"Total Market\"), (\"VOO\", \"ETF\", \"S&P 500\"),\n",
    "    (\"VEA\", \"ETF\", \"International\"), (\"VWO\", \"ETF\", \"Emerging\"), (\"AGG\", \"ETF\", \"Bonds\"),\n",
    "    (\"LQD\", \"ETF\", \"Corp Bonds\"), (\"HYG\", \"ETF\", \"High Yield\"), (\"TLT\", \"ETF\", \"Treasuries\"),\n",
    "    (\"IEF\", \"ETF\", \"Mid Treasuries\"), (\"SHY\", \"ETF\", \"Short Treasuries\"), (\"VNQ\", \"ETF\", \"Real Estate\"),\n",
    "    (\"XLRE\", \"ETF\", \"Real Estate\"), (\"XLK\", \"ETF\", \"Tech\"), (\"XLF\", \"ETF\", \"Financial\"),\n",
    "    (\"XLE\", \"ETF\", \"Energy\"), (\"XLV\", \"ETF\", \"Healthcare\"),\n",
    "    \n",
    "    # Sector ETFs (20)\n",
    "    (\"XLI\", \"ETF\", \"Industrial\"), (\"XLY\", \"ETF\", \"Consumer Disc\"), (\"XLP\", \"ETF\", \"Consumer Staples\"),\n",
    "    (\"XLB\", \"ETF\", \"Materials\"), (\"XLU\", \"ETF\", \"Utilities\"), (\"XLRE\", \"ETF\", \"Real Estate\"),\n",
    "    (\"VGT\", \"ETF\", \"Tech\"), (\"VDC\", \"ETF\", \"Consumer Staples\"), (\"VDE\", \"ETF\", \"Energy\"),\n",
    "    (\"VHT\", \"ETF\", \"Healthcare\"), (\"VIS\", \"ETF\", \"Industrial\"), (\"VAW\", \"ETF\", \"Materials\"),\n",
    "    (\"VPU\", \"ETF\", \"Utilities\"), (\"VFH\", \"ETF\", \"Financial\"), (\"VCR\", \"ETF\", \"Consumer Disc\"),\n",
    "    (\"VOX\", \"ETF\", \"Telecom\"), (\"IBB\", \"ETF\", \"Biotech\"), (\"SMH\", \"ETF\", \"Semiconductors\"),\n",
    "    (\"SOXX\", \"ETF\", \"Semiconductors\"), (\"ITB\", \"ETF\", \"Homebuilders\"),\n",
    "    \n",
    "    # Thematic ETFs (20)\n",
    "    (\"ARKK\", \"ETF\", \"Innovation\"), (\"ARKG\", \"ETF\", \"Genomics\"), (\"ARKW\", \"ETF\", \"Web\"),\n",
    "    (\"ARKF\", \"ETF\", \"Fintech\"), (\"ARKQ\", \"ETF\", \"Autonomous\"), (\"ICLN\", \"ETF\", \"Clean Energy\"),\n",
    "    (\"TAN\", \"ETF\", \"Solar\"), (\"LIT\", \"ETF\", \"Lithium\"), (\"REMX\", \"ETF\", \"Rare Earth\"),\n",
    "    (\"HACK\", \"ETF\", \"Cybersecurity\"), (\"ROBO\", \"ETF\", \"Robotics\"), (\"BOTZ\", \"ETF\", \"Robotics\"),\n",
    "    (\"CLOU\", \"ETF\", \"Cloud\"), (\"SKYY\", \"ETF\", \"Cloud\"), (\"JETS\", \"ETF\", \"Airlines\"),\n",
    "    (\"XHB\", \"ETF\", \"Homebuilders\"), (\"HERO\", \"ETF\", \"Gaming\"), (\"ESPO\", \"ETF\", \"Esports\"),\n",
    "    (\"BETZ\", \"ETF\", \"Gambling\"), (\"UFO\", \"ETF\", \"Space\"),\n",
    "    \n",
    "    # Commodities (30)\n",
    "    (\"GC=F\", \"Commodity\", \"Gold\"), (\"SI=F\", \"Commodity\", \"Silver\"), (\"PL=F\", \"Commodity\", \"Platinum\"),\n",
    "    (\"PA=F\", \"Commodity\", \"Palladium\"), (\"HG=F\", \"Commodity\", \"Copper\"), (\"CL=F\", \"Commodity\", \"Crude Oil\"),\n",
    "    (\"BZ=F\", \"Commodity\", \"Brent Oil\"), (\"NG=F\", \"Commodity\", \"Natural Gas\"), (\"RB=F\", \"Commodity\", \"Gasoline\"),\n",
    "    (\"HO=F\", \"Commodity\", \"Heating Oil\"), (\"ZC=F\", \"Commodity\", \"Corn\"), (\"ZW=F\", \"Commodity\", \"Wheat\"),\n",
    "    (\"ZS=F\", \"Commodity\", \"Soybeans\"), (\"KC=F\", \"Commodity\", \"Coffee\"), (\"CT=F\", \"Commodity\", \"Cotton\"),\n",
    "    (\"SB=F\", \"Commodity\", \"Sugar\"), (\"CC=F\", \"Commodity\", \"Cocoa\"), (\"OJ=F\", \"Commodity\", \"Orange Juice\"),\n",
    "    (\"LE=F\", \"Commodity\", \"Live Cattle\"), (\"HE=F\", \"Commodity\", \"Lean Hogs\"),\n",
    "    # Commodity ETFs as proxies\n",
    "    (\"GLD\", \"Commodity\", \"Gold ETF\"), (\"SLV\", \"Commodity\", \"Silver ETF\"), (\"PPLT\", \"Commodity\", \"Platinum ETF\"),\n",
    "    (\"PALL\", \"Commodity\", \"Palladium ETF\"), (\"COPX\", \"Commodity\", \"Copper ETF\"), (\"USO\", \"Commodity\", \"Oil ETF\"),\n",
    "    (\"UNG\", \"Commodity\", \"Gas ETF\"), (\"DBA\", \"Commodity\", \"Agriculture\"), (\"CORN\", \"Commodity\", \"Corn ETF\"),\n",
    "    (\"WEAT\", \"Commodity\", \"Wheat ETF\"),\n",
    "    \n",
    "    # Cryptocurrencies (20)\n",
    "    (\"BTC-USD\", \"Crypto\", \"Bitcoin\"), (\"ETH-USD\", \"Crypto\", \"Ethereum\"), (\"BNB-USD\", \"Crypto\", \"Binance Coin\"),\n",
    "    (\"XRP-USD\", \"Crypto\", \"Ripple\"), (\"ADA-USD\", \"Crypto\", \"Cardano\"), (\"DOGE-USD\", \"Crypto\", \"Dogecoin\"),\n",
    "    (\"SOL-USD\", \"Crypto\", \"Solana\"), (\"DOT-USD\", \"Crypto\", \"Polkadot\"), (\"MATIC-USD\", \"Crypto\", \"Polygon\"),\n",
    "    (\"LTC-USD\", \"Crypto\", \"Litecoin\"), (\"SHIB-USD\", \"Crypto\", \"Shiba Inu\"), (\"AVAX-USD\", \"Crypto\", \"Avalanche\"),\n",
    "    (\"UNI-USD\", \"Crypto\", \"Uniswap\"), (\"LINK-USD\", \"Crypto\", \"Chainlink\"), (\"ATOM-USD\", \"Crypto\", \"Cosmos\"),\n",
    "    (\"XLM-USD\", \"Crypto\", \"Stellar\"), (\"ALGO-USD\", \"Crypto\", \"Algorand\"), (\"VET-USD\", \"Crypto\", \"VeChain\"),\n",
    "    (\"ICP-USD\", \"Crypto\", \"Internet Computer\"), (\"FIL-USD\", \"Crypto\", \"Filecoin\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc9ab0c",
   "metadata": {},
   "source": [
    "Y Finance will make the scraping process a breeze. Lets define a simple scrape function than some logic to get our desired assets into /data. Lets get five years of historical classic OHLCV data for each asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e404bada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINANCIAL ASSET DATA SCRAPER\n",
      "================================================================================\n",
      "\n",
      "Date range: 2020-11-05 to 2025-11-04\n",
      "Target: 230 assets\n",
      "\n",
      "Processing: AAPL (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: MSFT (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: GOOGL (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: AMZN (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: NVDA (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: META (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: TSLA (Stock - Auto)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: BRK-B (Stock - Financial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: V (Stock - Financial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: JNJ (Stock - Healthcare)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: WMT (Stock - Retail)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: JPM (Stock - Financial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: MA (Stock - Financial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: PG (Stock - Consumer)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: UNH (Stock - Healthcare)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: HD (Stock - Retail)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: DIS (Stock - Media)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: BAC (Stock - Financial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: ADBE (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: CRM (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: NFLX (Stock - Media)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: CMCSA (Stock - Telecom)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: PEP (Stock - Consumer)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: COST (Stock - Retail)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: CSCO (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: AVGO (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: INTC (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: AMD (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: QCOM (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: TXN (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: INTU (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: AMAT (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: BKNG (Stock - Travel)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: AMGN (Stock - Healthcare)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: SBUX (Stock - Consumer)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: GILD (Stock - Healthcare)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: MDLZ (Stock - Consumer)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: ADP (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: ISRG (Stock - Healthcare)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: LRCX (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: ADI (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: REGN (Stock - Healthcare)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: PYPL (Stock - Financial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: MU (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: VRTX (Stock - Healthcare)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: MRNA (Stock - Healthcare)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: NXPI (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: KLAC (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: MELI (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: ASML (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: ABNB (Stock - Travel)\n",
      "  [OK] Downloaded 1231 data points\n",
      "\n",
      "Processing: COIN (Stock - Financial)\n",
      "  [OK] Downloaded 1147 data points\n",
      "\n",
      "Processing: ROKU (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: SNAP (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: SPOT (Stock - Media)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['SQ']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: SQ (Stock - Financial)\n",
      "  [FAIL] Failed or insufficient data\n",
      "\n",
      "Processing: RBLX (Stock - Tech)\n",
      "  [OK] Downloaded 1171 data points\n",
      "\n",
      "Processing: PLTR (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: SNOW (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: NET (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: CRWD (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: ZS (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: DDOG (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: MDB (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: TEAM (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: ZM (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: DOCU (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: UBER (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: LYFT (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: DASH (Stock - Tech)\n",
      "  [OK] Downloaded 1232 data points\n",
      "\n",
      "Processing: PINS (Stock - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: ETSY (Stock - Retail)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: W (Stock - Retail)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: CHWY (Stock - Retail)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: PTON (Stock - Consumer)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: RIVN (Stock - Auto)\n",
      "  [OK] Downloaded 1000 data points\n",
      "\n",
      "Processing: LCID (Stock - Auto)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: F (Stock - Auto)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: GM (Stock - Auto)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: NIO (Stock - Auto)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: XOM (Stock - Energy)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: CVX (Stock - Energy)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: COP (Stock - Energy)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: SLB (Stock - Energy)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: EOG (Stock - Energy)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: MPC (Stock - Energy)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: PSX (Stock - Energy)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: VLO (Stock - Energy)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: OXY (Stock - Energy)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: HAL (Stock - Energy)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: BKR (Stock - Energy)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: DVN (Stock - Energy)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['HES']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "\n",
      "1 Failed download:\n",
      "['MRO']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: FANG (Stock - Energy)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: HES (Stock - Energy)\n",
      "  [FAIL] Failed or insufficient data\n",
      "\n",
      "Processing: MRO (Stock - Energy)\n",
      "  [FAIL] Failed or insufficient data\n",
      "\n",
      "Processing: APA (Stock - Energy)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: CTRA (Stock - Energy)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: OVV (Stock - Energy)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: NEE (Stock - Energy)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: DUK (Stock - Energy)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: GS (Stock - Financial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: MS (Stock - Financial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: C (Stock - Financial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: WFC (Stock - Financial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: USB (Stock - Financial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: PNC (Stock - Financial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: TFC (Stock - Financial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: SCHW (Stock - Financial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: BLK (Stock - Financial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: AXP (Stock - Financial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: SPGI (Stock - Financial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: MCO (Stock - Financial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: ICE (Stock - Financial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: CME (Stock - Financial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: BX (Stock - Financial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: KKR (Stock - Financial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: APO (Stock - Financial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: COF (Stock - Financial)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "['DFS']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: DFS (Stock - Financial)\n",
      "  [FAIL] Failed or insufficient data\n",
      "\n",
      "Processing: SYF (Stock - Financial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: SPY (ETF - Broad Market)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: QQQ (ETF - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: IWM (ETF - Small Cap)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: DIA (ETF - Blue Chip)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: VTI (ETF - Total Market)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: VOO (ETF - S&P 500)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: VEA (ETF - International)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: VWO (ETF - Emerging)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: AGG (ETF - Bonds)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: LQD (ETF - Corp Bonds)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: HYG (ETF - High Yield)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: TLT (ETF - Treasuries)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: IEF (ETF - Mid Treasuries)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: SHY (ETF - Short Treasuries)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: VNQ (ETF - Real Estate)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: XLRE (ETF - Real Estate)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: XLK (ETF - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: XLF (ETF - Financial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: XLE (ETF - Energy)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: XLV (ETF - Healthcare)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: XLI (ETF - Industrial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: XLY (ETF - Consumer Disc)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: XLP (ETF - Consumer Staples)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: XLB (ETF - Materials)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: XLU (ETF - Utilities)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: XLRE (ETF - Real Estate)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: VGT (ETF - Tech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: VDC (ETF - Consumer Staples)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: VDE (ETF - Energy)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: VHT (ETF - Healthcare)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: VIS (ETF - Industrial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: VAW (ETF - Materials)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: VPU (ETF - Utilities)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: VFH (ETF - Financial)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: VCR (ETF - Consumer Disc)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: VOX (ETF - Telecom)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: IBB (ETF - Biotech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: SMH (ETF - Semiconductors)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: SOXX (ETF - Semiconductors)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: ITB (ETF - Homebuilders)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: ARKK (ETF - Innovation)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: ARKG (ETF - Genomics)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: ARKW (ETF - Web)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: ARKF (ETF - Fintech)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: ARKQ (ETF - Autonomous)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: ICLN (ETF - Clean Energy)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: TAN (ETF - Solar)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: LIT (ETF - Lithium)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: REMX (ETF - Rare Earth)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: HACK (ETF - Cybersecurity)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: ROBO (ETF - Robotics)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: BOTZ (ETF - Robotics)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: CLOU (ETF - Cloud)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: SKYY (ETF - Cloud)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: JETS (ETF - Airlines)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: XHB (ETF - Homebuilders)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: HERO (ETF - Gaming)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: ESPO (ETF - Esports)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: BETZ (ETF - Gambling)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: UFO (ETF - Space)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: GC=F (Commodity - Gold)\n",
      "  [OK] Downloaded 1256 data points\n",
      "\n",
      "Processing: SI=F (Commodity - Silver)\n",
      "  [OK] Downloaded 1256 data points\n",
      "\n",
      "Processing: PL=F (Commodity - Platinum)\n",
      "  [OK] Downloaded 1256 data points\n",
      "\n",
      "Processing: PA=F (Commodity - Palladium)\n",
      "  [OK] Downloaded 1255 data points\n",
      "\n",
      "Processing: HG=F (Commodity - Copper)\n",
      "  [OK] Downloaded 1257 data points\n",
      "\n",
      "Processing: CL=F (Commodity - Crude Oil)\n",
      "  [OK] Downloaded 1256 data points\n",
      "\n",
      "Processing: BZ=F (Commodity - Brent Oil)\n",
      "  [OK] Downloaded 1257 data points\n",
      "\n",
      "Processing: NG=F (Commodity - Natural Gas)\n",
      "  [OK] Downloaded 1257 data points\n",
      "\n",
      "Processing: RB=F (Commodity - Gasoline)\n",
      "  [OK] Downloaded 1257 data points\n",
      "\n",
      "Processing: HO=F (Commodity - Heating Oil)\n",
      "  [OK] Downloaded 1257 data points\n",
      "\n",
      "Processing: ZC=F (Commodity - Corn)\n",
      "  [OK] Downloaded 1255 data points\n",
      "\n",
      "Processing: ZW=F (Commodity - Wheat)\n",
      "  [OK] Downloaded 1255 data points\n",
      "\n",
      "Processing: ZS=F (Commodity - Soybeans)\n",
      "  [OK] Downloaded 1255 data points\n",
      "\n",
      "Processing: KC=F (Commodity - Coffee)\n",
      "  [OK] Downloaded 1257 data points\n",
      "\n",
      "Processing: CT=F (Commodity - Cotton)\n",
      "  [OK] Downloaded 1257 data points\n",
      "\n",
      "Processing: SB=F (Commodity - Sugar)\n",
      "  [OK] Downloaded 1257 data points\n",
      "\n",
      "Processing: CC=F (Commodity - Cocoa)\n",
      "  [OK] Downloaded 1256 data points\n",
      "\n",
      "Processing: OJ=F (Commodity - Orange Juice)\n",
      "  [OK] Downloaded 1257 data points\n",
      "\n",
      "Processing: LE=F (Commodity - Live Cattle)\n",
      "  [OK] Downloaded 1255 data points\n",
      "\n",
      "Processing: HE=F (Commodity - Lean Hogs)\n",
      "  [OK] Downloaded 1255 data points\n",
      "\n",
      "Processing: GLD (Commodity - Gold ETF)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: SLV (Commodity - Silver ETF)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: PPLT (Commodity - Platinum ETF)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: PALL (Commodity - Palladium ETF)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: COPX (Commodity - Copper ETF)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: USO (Commodity - Oil ETF)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: UNG (Commodity - Gas ETF)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: DBA (Commodity - Agriculture)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: CORN (Commodity - Corn ETF)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: WEAT (Commodity - Wheat ETF)\n",
      "  [OK] Downloaded 1254 data points\n",
      "\n",
      "Processing: BTC-USD (Crypto - Bitcoin)\n",
      "  [OK] Downloaded 1825 data points\n",
      "\n",
      "Processing: ETH-USD (Crypto - Ethereum)\n",
      "  [OK] Downloaded 1825 data points\n",
      "\n",
      "Processing: BNB-USD (Crypto - Binance Coin)\n",
      "  [OK] Downloaded 1825 data points\n",
      "\n",
      "Processing: XRP-USD (Crypto - Ripple)\n",
      "  [OK] Downloaded 1825 data points\n",
      "\n",
      "Processing: ADA-USD (Crypto - Cardano)\n",
      "  [OK] Downloaded 1825 data points\n",
      "\n",
      "Processing: DOGE-USD (Crypto - Dogecoin)\n",
      "  [OK] Downloaded 1825 data points\n",
      "\n",
      "Processing: SOL-USD (Crypto - Solana)\n",
      "  [OK] Downloaded 1825 data points\n",
      "\n",
      "Processing: DOT-USD (Crypto - Polkadot)\n",
      "  [OK] Downloaded 1825 data points\n",
      "\n",
      "Processing: MATIC-USD (Crypto - Polygon)\n",
      "  [OK] Downloaded 1601 data points\n",
      "\n",
      "Processing: LTC-USD (Crypto - Litecoin)\n",
      "  [OK] Downloaded 1825 data points\n",
      "\n",
      "Processing: SHIB-USD (Crypto - Shiba Inu)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def fetch_historical_data(ticker, start_date, end_date):\n",
    "    try:\n",
    "        data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "        if data.empty:\n",
    "            return None\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"  Error fetching data for {ticker}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FINANCIAL ASSET DATA SCRAPER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 5 years of data\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=5*365)\n",
    "\n",
    "print(f\"\\nDate range: {start_date.date()} to {end_date.date()}\")\n",
    "print(f\"Target: {len(FINANCIAL_ASSETS)} assets\")\n",
    "\n",
    "asset_data = []\n",
    "failed_tickers = []\n",
    "\n",
    "for ticker, asset_class, category in FINANCIAL_ASSETS:\n",
    "    print(f\"\\nProcessing: {ticker} ({asset_class} - {category})\")\n",
    "    \n",
    "    data = fetch_historical_data(ticker, start_date, end_date)\n",
    "    \n",
    "    if data is not None and len(data) > 100:  # Require at least 100 data points\n",
    "        asset_data.append({\n",
    "            'ticker': ticker,\n",
    "            'asset_class': asset_class,\n",
    "            'category': category,\n",
    "            'data': data,\n",
    "            'start': data.index[0],\n",
    "            'end': data.index[-1],\n",
    "            'count': len(data)\n",
    "        })\n",
    "        print(f\"  [OK] Downloaded {len(data)} data points\")\n",
    "    else:\n",
    "        failed_tickers.append(ticker)\n",
    "        print(f\"  [FAIL] Failed or insufficient data\")\n",
    "\n",
    "print(f\"\\n\\nSuccessfully downloaded: {len(asset_data)} assets\")\n",
    "print(f\"Failed: {len(failed_tickers)} assets\")\n",
    "\n",
    "if failed_tickers:\n",
    "    print(\"\\nFailed tickers:\", \", \".join(failed_tickers))\n",
    "\n",
    "# Find common date range\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 2: FINDING COMMON DATE RANGE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not asset_data:\n",
    "    print(\"ERROR: No data downloaded successfully!\")\n",
    "    exit(1)\n",
    "\n",
    "# Find the latest start date and earliest end date\n",
    "latest_start = max(asset['start'] for asset in asset_data)\n",
    "earliest_end = min(asset['end'] for asset in asset_data)\n",
    "\n",
    "print(f\"\\nCommon date range: {latest_start.date()} to {earliest_end.date()}\")\n",
    "\n",
    "# Filter assets that have data for the full common range\n",
    "filtered_assets = []\n",
    "for asset in asset_data:\n",
    "    # Check if asset covers the common range\n",
    "    if asset['start'] <= latest_start and asset['end'] >= earliest_end:\n",
    "        # Trim data to common range\n",
    "        asset['data'] = asset['data'].loc[latest_start:earliest_end]\n",
    "        filtered_assets.append(asset)\n",
    "        print(f\"  [OK] {asset['ticker']}: {len(asset['data'])} points in common range\")\n",
    "    else:\n",
    "        print(f\"  [SKIP] {asset['ticker']}: Insufficient coverage\")\n",
    "\n",
    "print(f\"\\n\\nAssets with full date range: {len(filtered_assets)}\")\n",
    "\n",
    "# Save data\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PHASE 4: SAVING DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create summary table\n",
    "summary_data = []\n",
    "for asset in filtered_assets:\n",
    "    summary_data.append({\n",
    "        'Ticker': asset['ticker'],\n",
    "        'Asset_Class': asset['asset_class'],\n",
    "        'Category': asset['category'],\n",
    "        'Data_Points': len(asset['data']),\n",
    "        'Start_Date': asset['data'].index[0].strftime('%Y-%m-%d'),\n",
    "        'End_Date': asset['data'].index[-1].strftime('%Y-%m-%d'),\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df.to_csv(DATA_DIR / \"asset_summary.csv\", index=False)\n",
    "print(f\"[OK] Saved asset summary: {len(summary_df)} assets\")\n",
    "\n",
    "# Save individual price data\n",
    "for asset in filtered_assets:\n",
    "    ticker_clean = asset['ticker'].replace('=F', '').replace('-USD', '').replace('/', '_')\n",
    "    csv_path = DATA_DIR / f\"{ticker_clean}.csv\"\n",
    "    asset['data'].to_csv(csv_path)\n",
    "\n",
    "print(f\"[OK] Saved {len(filtered_assets)} individual price files\")\n",
    "\n",
    "# Create combined price matrix (closing prices)\n",
    "print(\"\\nCreating combined price matrix...\")\n",
    "price_matrix = pd.DataFrame()\n",
    "for asset in filtered_assets:\n",
    "    ticker = asset['ticker']\n",
    "    # Get close price, handle multi-index if present\n",
    "    if isinstance(asset['data'].columns, pd.MultiIndex):\n",
    "        close_col = asset['data']['Close'].iloc[:, 0] if len(asset['data']['Close'].shape) > 1 else asset['data']['Close']\n",
    "    else:\n",
    "        close_col = asset['data']['Close']\n",
    "    price_matrix[ticker] = close_col\n",
    "\n",
    "price_matrix.to_csv(DATA_DIR / \"price_matrix.csv\")\n",
    "print(f\"[OK] Saved combined price matrix: {price_matrix.shape}\")\n",
    "\n",
    "# Calculate returns\n",
    "returns_matrix = price_matrix.pct_change().dropna()\n",
    "returns_matrix.to_csv(DATA_DIR / \"returns_matrix.csv\")\n",
    "print(f\"[OK] Saved returns matrix: {returns_matrix.shape}\")\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal assets collected: {len(filtered_assets)}\")\n",
    "print(f\"Date range: {latest_start.date()} to {earliest_end.date()}\")\n",
    "print(f\"Data points per asset: {len(filtered_assets[0]['data']) if filtered_assets else 0}\")\n",
    "print(\"\\nAsset class distribution:\")\n",
    "print(summary_df['Asset_Class'].value_counts())\n",
    "print(\"\\nData saved to:\", DATA_DIR)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b631a8",
   "metadata": {},
   "source": [
    "# Feature Engineering for Clustering\n",
    "\n",
    "Now we need to transform the returns matrix into a proper feature matrix for clustering. Each **ticker** should be a row, and each **feature** (statistical measure) should be a column. This will allow us to cluster assets based on similar risk/return profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83855595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE ENGINEERING FOR CLUSTERING\n",
      "================================================================================\n",
      "\n",
      "Returns matrix shape: (843, 225)\n",
      "  843 dates x 225 tickers\n",
      "\n",
      "âœ“ Feature matrix created: (225, 12)\n",
      "  225 tickers x 12 features\n",
      "\n",
      "[OK] Saved clustering feature matrix to: data\\clustering_features.csv\n",
      "\n",
      "================================================================================\n",
      "SAMPLE FEATURES (first 5 assets)\n",
      "================================================================================\n",
      "        Mean_Return  Volatility  Sharpe_Ratio  Skewness  Kurtosis  \\\n",
      "Ticker                                                              \n",
      "AAPL       0.162052    0.273068      0.593449  0.135848  2.134865   \n",
      "MSFT       0.097622    0.273047      0.357529 -0.016575  1.951621   \n",
      "GOOGL      0.095064    0.322573      0.294704 -0.038350  2.720047   \n",
      "AMZN       0.115554    0.372382      0.310310  0.082347  4.409861   \n",
      "NVDA       0.581699    0.564121      1.031159  0.454702  3.755531   \n",
      "\n",
      "        Downside_Volatility  Max_Drawdown  Percentile_5  Percentile_95  \\\n",
      "Ticker                                                                   \n",
      "AAPL               0.183012     -0.309128     -0.028379       0.026669   \n",
      "MSFT               0.185937     -0.371485     -0.027116       0.027027   \n",
      "GOOGL              0.221504     -0.443200     -0.031277       0.030394   \n",
      "AMZN               0.253337     -0.557258     -0.035615       0.035913   \n",
      "NVDA               0.341733     -0.663351     -0.055414       0.055513   \n",
      "\n",
      "        Rolling_Volatility  Win_Rate  Gain_Loss_Ratio  \n",
      "Ticker                                                 \n",
      "AAPL              0.260163  0.529063         0.977640  \n",
      "MSFT              0.265340  0.516014         0.996642  \n",
      "GOOGL             0.316398  0.520759         0.963109  \n",
      "AMZN              0.355825  0.505338         1.025704  \n",
      "NVDA              0.542911  0.531435         1.043478  \n",
      "\n",
      "================================================================================\n",
      "FEATURE STATISTICS\n",
      "================================================================================\n",
      "       Mean_Return   Volatility  Sharpe_Ratio    Skewness    Kurtosis  \\\n",
      "count   225.000000   225.000000    225.000000  225.000000  225.000000   \n",
      "mean     21.003215    38.642915      0.251177    0.207826    8.584913   \n",
      "std     313.993446   573.688142      0.396720    2.049860   56.008724   \n",
      "min      -0.501966     0.022499     -0.610546   -1.802087    0.093826   \n",
      "25%      -0.002706     0.245835     -0.007470   -0.197872    1.388401   \n",
      "50%       0.074020     0.332237      0.256776    0.039366    2.528033   \n",
      "75%       0.162052     0.473378      0.523459    0.247106    5.497474   \n",
      "max    4709.971372  8605.718491      1.234947   28.982761  838.000670   \n",
      "\n",
      "       Downside_Volatility  Max_Drawdown  Percentile_5  Percentile_95  \\\n",
      "count           225.000000    225.000000    225.000000     225.000000   \n",
      "mean              0.272071     -0.502182     -0.038275       0.039088   \n",
      "std               0.185063      0.229163      0.020814       0.021798   \n",
      "min               0.013962     -0.999834     -0.100000       0.002227   \n",
      "25%               0.165876     -0.675808     -0.044732       0.023620   \n",
      "50%               0.222807     -0.453026     -0.032844       0.033135   \n",
      "75%               0.325836     -0.323774     -0.024256       0.048755   \n",
      "max               1.994554     -0.052514     -0.002092       0.110000   \n",
      "\n",
      "       Rolling_Volatility    Win_Rate  Gain_Loss_Ratio  \n",
      "count          225.000000  225.000000       225.000000  \n",
      "mean             7.859300    0.505686         4.360318  \n",
      "std            112.250413    0.031068        50.260036  \n",
      "min              0.021368    0.192171         0.855449  \n",
      "25%              0.235339    0.493476         0.970004  \n",
      "50%              0.315691    0.507711         1.002765  \n",
      "75%              0.446563    0.521945         1.045598  \n",
      "max           1684.129189    0.570581       754.909613  \n",
      "\n",
      "âœ“ Data is now READY for clustering!\n",
      "  - Each row = one asset\n",
      "  - Each column = a risk/return feature\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE ENGINEERING FOR CLUSTERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load returns matrix\n",
    "returns_df = pd.read_csv(DATA_DIR / \"returns_matrix.csv\", index_col=0, parse_dates=True)\n",
    "print(f\"\\nReturns matrix shape: {returns_df.shape}\")\n",
    "print(f\"  {len(returns_df)} dates x {len(returns_df.columns)} tickers\")\n",
    "\n",
    "# Initialize feature dataframe\n",
    "features = []\n",
    "\n",
    "# Calculate features for each ticker\n",
    "for ticker in returns_df.columns:\n",
    "    returns = returns_df[ticker].dropna()\n",
    "    \n",
    "    # Basic return statistics\n",
    "    mean_return = returns.mean() * 252  # Annualized\n",
    "    std_return = returns.std() * np.sqrt(252)  # Annualized volatility\n",
    "    \n",
    "    # Risk-adjusted return\n",
    "    sharpe_ratio = mean_return / std_return if std_return != 0 else 0\n",
    "    \n",
    "    # Distribution statistics\n",
    "    returns_skew = skew(returns)\n",
    "    returns_kurtosis = kurtosis(returns)\n",
    "    \n",
    "    # Downside risk\n",
    "    negative_returns = returns[returns < 0]\n",
    "    downside_std = negative_returns.std() * np.sqrt(252) if len(negative_returns) > 0 else 0\n",
    "    \n",
    "    # Maximum drawdown\n",
    "    cumulative = (1 + returns).cumprod()\n",
    "    running_max = cumulative.expanding().max()\n",
    "    drawdown = (cumulative - running_max) / running_max\n",
    "    max_drawdown = drawdown.min()\n",
    "    \n",
    "    # Percentile returns\n",
    "    percentile_5 = returns.quantile(0.05)\n",
    "    percentile_95 = returns.quantile(0.95)\n",
    "    \n",
    "    # Rolling volatility (30-day)\n",
    "    rolling_vol = returns.rolling(30).std().mean() * np.sqrt(252)\n",
    "    \n",
    "    # Win rate\n",
    "    win_rate = (returns > 0).sum() / len(returns)\n",
    "    \n",
    "    # Average gain vs average loss\n",
    "    avg_gain = returns[returns > 0].mean() if (returns > 0).sum() > 0 else 0\n",
    "    avg_loss = abs(returns[returns < 0].mean()) if (returns < 0).sum() > 0 else 0\n",
    "    gain_loss_ratio = avg_gain / avg_loss if avg_loss != 0 else 0\n",
    "    \n",
    "    features.append({\n",
    "        'Ticker': ticker,\n",
    "        'Mean_Return': mean_return,\n",
    "        'Volatility': std_return,\n",
    "        'Sharpe_Ratio': sharpe_ratio,\n",
    "        'Skewness': returns_skew,\n",
    "        'Kurtosis': returns_kurtosis,\n",
    "        'Downside_Volatility': downside_std,\n",
    "        'Max_Drawdown': max_drawdown,\n",
    "        'Percentile_5': percentile_5,\n",
    "        'Percentile_95': percentile_95,\n",
    "        'Rolling_Volatility': rolling_vol,\n",
    "        'Win_Rate': win_rate,\n",
    "        'Gain_Loss_Ratio': gain_loss_ratio\n",
    "    })\n",
    "\n",
    "# Create feature dataframe\n",
    "feature_df = pd.DataFrame(features)\n",
    "feature_df.set_index('Ticker', inplace=True)\n",
    "\n",
    "print(f\"\\nâœ“ Feature matrix created: {feature_df.shape}\")\n",
    "print(f\"  {len(feature_df)} tickers x {len(feature_df.columns)} features\")\n",
    "\n",
    "# Save feature matrix\n",
    "feature_df.to_csv(DATA_DIR / \"clustering_features.csv\")\n",
    "print(f\"\\n[OK] Saved clustering feature matrix to: {DATA_DIR / 'clustering_features.csv'}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE FEATURES (first 5 assets)\")\n",
    "print(\"=\" * 80)\n",
    "print(feature_df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FEATURE STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "print(feature_df.describe())\n",
    "\n",
    "print(\"\\nâœ“ Data is now READY for clustering!\")\n",
    "print(\"  - Each row = one asset\")\n",
    "print(\"  - Each column = a risk/return feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3aa22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CALCULATING MARKET CORRELATIONS\n",
      "================================================================================\n",
      "\n",
      "[OK] Added market correlation features\n",
      "  - Correlation with SPY\n",
      "  - Beta relative to SPY\n",
      "\n",
      "âœ“ Final feature matrix: (225, 14)\n",
      "        Mean_Return  Volatility  Sharpe_Ratio  Skewness  Kurtosis  \\\n",
      "Ticker                                                              \n",
      "AAPL       0.162052    0.273068      0.593449  0.135848  2.134865   \n",
      "MSFT       0.097622    0.273047      0.357529 -0.016575  1.951621   \n",
      "GOOGL      0.095064    0.322573      0.294704 -0.038350  2.720047   \n",
      "AMZN       0.115554    0.372382      0.310310  0.082347  4.409861   \n",
      "NVDA       0.581699    0.564121      1.031159  0.454702  3.755531   \n",
      "\n",
      "        Downside_Volatility  Max_Drawdown  Percentile_5  Percentile_95  \\\n",
      "Ticker                                                                   \n",
      "AAPL               0.183012     -0.309128     -0.028379       0.026669   \n",
      "MSFT               0.185937     -0.371485     -0.027116       0.027027   \n",
      "GOOGL              0.221504     -0.443200     -0.031277       0.030394   \n",
      "AMZN               0.253337     -0.557258     -0.035615       0.035913   \n",
      "NVDA               0.341733     -0.663351     -0.055414       0.055513   \n",
      "\n",
      "        Rolling_Volatility  Win_Rate  Gain_Loss_Ratio  Correlation_SPY  \\\n",
      "Ticker                                                                   \n",
      "AAPL              0.260163  0.529063         0.977640         0.755686   \n",
      "MSFT              0.265340  0.516014         0.996642         0.782846   \n",
      "GOOGL             0.316398  0.520759         0.963109         0.710243   \n",
      "AMZN              0.355825  0.505338         1.025704         0.730419   \n",
      "NVDA              0.542911  0.531435         1.043478         0.711732   \n",
      "\n",
      "        Beta_SPY  \n",
      "Ticker            \n",
      "AAPL    1.188718  \n",
      "MSFT    1.231351  \n",
      "GOOGL   1.319783  \n",
      "AMZN    1.566853  \n",
      "NVDA    2.312898  \n"
     ]
    }
   ],
   "source": [
    "# Add market correlation features (Beta calculation)\n",
    "print(\"=\" * 80)\n",
    "print(\"CALCULATING MARKET CORRELATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use SPY as market proxy\n",
    "if 'SPY' in returns_df.columns:\n",
    "    market_returns = returns_df['SPY']\n",
    "    \n",
    "    correlation_features = []\n",
    "    for ticker in returns_df.columns:\n",
    "        if ticker == 'SPY':\n",
    "            corr_spy = 1.0\n",
    "            beta_spy = 1.0\n",
    "        else:\n",
    "            ticker_returns = returns_df[ticker].dropna()\n",
    "            aligned = pd.concat([ticker_returns, market_returns], axis=1).dropna()\n",
    "            \n",
    "            if len(aligned) > 1:\n",
    "                # Correlation with SPY\n",
    "                corr_spy = aligned.corr().iloc[0, 1]\n",
    "                \n",
    "                # Beta calculation: Cov(asset, market) / Var(market)\n",
    "                covariance = aligned.cov().iloc[0, 1]\n",
    "                market_variance = aligned['SPY'].var()\n",
    "                beta_spy = covariance / market_variance if market_variance != 0 else 0\n",
    "            else:\n",
    "                corr_spy = 0\n",
    "                beta_spy = 0\n",
    "        \n",
    "        correlation_features.append({\n",
    "            'Ticker': ticker,\n",
    "            'Correlation_SPY': corr_spy,\n",
    "            'Beta_SPY': beta_spy\n",
    "        })\n",
    "    \n",
    "    corr_df = pd.DataFrame(correlation_features).set_index('Ticker')\n",
    "    \n",
    "    # Remove existing correlation columns if present (for re-runs)\n",
    "    cols_to_drop = [col for col in ['Correlation_SPY', 'Beta_SPY'] if col in feature_df.columns]\n",
    "    if cols_to_drop:\n",
    "        feature_df = feature_df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    # Merge with existing features\n",
    "    feature_df = feature_df.join(corr_df)\n",
    "    \n",
    "    # Save updated features\n",
    "    feature_df.to_csv(DATA_DIR / \"clustering_features.csv\")\n",
    "    print(f\"\\n[OK] Added market correlation features\")\n",
    "    print(f\"  - Correlation with SPY\")\n",
    "    print(f\"  - Beta relative to SPY\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Final feature matrix: {feature_df.shape}\")\n",
    "    print(feature_df.head())\n",
    "else:\n",
    "    print(\"\\nâš  SPY not found in data - skipping market correlation features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58615d0",
   "metadata": {},
   "source": [
    "# Create Master Dataset\n",
    "\n",
    "Combine all features with asset metadata into a single CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5552038c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MASTER DATASET CREATED\n",
      "================================================================================\n",
      "\n",
      "âœ“ Saved: asset_clustering_data.csv\n",
      "  Shape: (226, 17)\n",
      "  226 assets Ã— 17 columns\n",
      "\n",
      "  Metadata: Ticker, Asset_Class, Category\n",
      "  Features: 14 risk/return metrics\n",
      "\n",
      "First few rows:\n",
      "  Ticker Asset_Class Category  Mean_Return  Volatility  Sharpe_Ratio  \\\n",
      "0   AAPL       Stock     Tech     0.162052    0.273068      0.593449   \n",
      "1   MSFT       Stock     Tech     0.097622    0.273047      0.357529   \n",
      "2  GOOGL       Stock     Tech     0.095064    0.322573      0.294704   \n",
      "3   AMZN       Stock     Tech     0.115554    0.372382      0.310310   \n",
      "4   NVDA       Stock     Tech     0.581699    0.564121      1.031159   \n",
      "\n",
      "   Skewness  Kurtosis  Downside_Volatility  Max_Drawdown  Percentile_5  \\\n",
      "0  0.135848  2.134865             0.183012     -0.309128     -0.028379   \n",
      "1 -0.016575  1.951621             0.185937     -0.371485     -0.027116   \n",
      "2 -0.038350  2.720047             0.221504     -0.443200     -0.031277   \n",
      "3  0.082347  4.409861             0.253337     -0.557258     -0.035615   \n",
      "4  0.454702  3.755531             0.341733     -0.663351     -0.055414   \n",
      "\n",
      "   Percentile_95  Rolling_Volatility  Win_Rate  Gain_Loss_Ratio  \\\n",
      "0       0.026669            0.260163  0.529063         0.977640   \n",
      "1       0.027027            0.265340  0.516014         0.996642   \n",
      "2       0.030394            0.316398  0.520759         0.963109   \n",
      "3       0.035913            0.355825  0.505338         1.025704   \n",
      "4       0.055513            0.542911  0.531435         1.043478   \n",
      "\n",
      "   Correlation_SPY  Beta_SPY  \n",
      "0         0.755686  1.188718  \n",
      "1         0.782846  1.231351  \n",
      "2         0.710243  1.319783  \n",
      "3         0.730419  1.566853  \n",
      "4         0.711732  2.312898  \n"
     ]
    }
   ],
   "source": [
    "# Load feature matrix and asset summary\n",
    "features = pd.read_csv(DATA_DIR / \"clustering_features.csv\", index_col=0)\n",
    "summary = pd.read_csv(DATA_DIR / \"asset_summary.csv\")\n",
    "\n",
    "# Merge on ticker\n",
    "master_df = features.reset_index().rename(columns={'Ticker': 'Ticker'})\n",
    "master_df = master_df.merge(summary, on='Ticker', how='left')\n",
    "\n",
    "# Reorder columns: Ticker, Asset_Class, Category, then features\n",
    "cols = ['Ticker', 'Asset_Class', 'Category'] + [col for col in features.columns]\n",
    "master_df = master_df[cols]\n",
    "\n",
    "# Save to root directory\n",
    "master_df.to_csv(\"asset_clustering_data.csv\", index=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MASTER DATASET CREATED\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nâœ“ Saved: asset_clustering_data.csv\")\n",
    "print(f\"  Shape: {master_df.shape}\")\n",
    "print(f\"  {len(master_df)} assets Ã— {len(master_df.columns)} columns\")\n",
    "print(f\"\\n  Metadata: Ticker, Asset_Class, Category\")\n",
    "print(f\"  Features: {len(features.columns)} risk/return metrics\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(master_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3243ff19",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "Understand the feature distributions and identify outliers before clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6239bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXPLORATORY DATA ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Dataset shape: (225, 14)\n",
      "Missing values: 0\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "FEATURE STATISTICS\n",
      "--------------------------------------------------------------------------------\n",
      "       Mean_Return   Volatility  Sharpe_Ratio    Skewness    Kurtosis  \\\n",
      "count   225.000000   225.000000    225.000000  225.000000  225.000000   \n",
      "mean     21.003215    38.642915      0.251177    0.207826    8.584913   \n",
      "std     313.993446   573.688142      0.396720    2.049860   56.008724   \n",
      "min      -0.501966     0.022499     -0.610546   -1.802087    0.093826   \n",
      "25%      -0.002706     0.245835     -0.007470   -0.197872    1.388401   \n",
      "50%       0.074020     0.332237      0.256776    0.039366    2.528033   \n",
      "75%       0.162052     0.473378      0.523459    0.247106    5.497474   \n",
      "max    4709.971372  8605.718491      1.234947   28.982761  838.000670   \n",
      "\n",
      "       Downside_Volatility  Max_Drawdown  Percentile_5  Percentile_95  \\\n",
      "count           225.000000    225.000000    225.000000     225.000000   \n",
      "mean              0.272071     -0.502182     -0.038275       0.039088   \n",
      "std               0.185063      0.229163      0.020814       0.021798   \n",
      "min               0.013962     -0.999834     -0.100000       0.002227   \n",
      "25%               0.165876     -0.675808     -0.044732       0.023620   \n",
      "50%               0.222807     -0.453026     -0.032844       0.033135   \n",
      "75%               0.325836     -0.323774     -0.024256       0.048755   \n",
      "max               1.994554     -0.052514     -0.002092       0.110000   \n",
      "\n",
      "       Rolling_Volatility    Win_Rate  Gain_Loss_Ratio  Correlation_SPY  \\\n",
      "count          225.000000  225.000000       225.000000       225.000000   \n",
      "mean             7.859300    0.505686         4.360318         0.510020   \n",
      "std            112.250413    0.031068        50.260036         0.242266   \n",
      "min              0.021368    0.192171         0.855449        -0.037766   \n",
      "25%              0.235339    0.493476         0.970004         0.350176   \n",
      "50%              0.315691    0.507711         1.002765         0.550596   \n",
      "75%              0.446563    0.521945         1.045598         0.698477   \n",
      "max           1684.129189    0.570581       754.909613         1.000000   \n",
      "\n",
      "          Beta_SPY  \n",
      "count   225.000000  \n",
      "mean     17.733859  \n",
      "std     249.380108  \n",
      "min      -0.059411  \n",
      "25%       0.616331  \n",
      "50%       1.103688  \n",
      "75%       1.626467  \n",
      "max    3741.797823  \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "OUTLIER DETECTION (3Ã—IQR method)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Mean_Return: 3 outliers\n",
      "  PLTR: 0.6748\n",
      "  LCID: -0.5020\n",
      "  UNI-USD: 4709.9714\n",
      "\n",
      "Volatility: 2 outliers\n",
      "  SHIB-USD: 1.1937\n",
      "  UNI-USD: 8605.7185\n",
      "\n",
      "Skewness: 8 outliers\n",
      "  NFLX: -1.8021\n",
      "  SBUX: 1.8561\n",
      "  CT=F: -1.7407\n",
      "  XRP-USD: 3.7316\n",
      "  DOGE-USD: 2.6526\n",
      "\n",
      "Kurtosis: 13 outliers\n",
      "  META: 20.8047\n",
      "  NFLX: 26.9994\n",
      "  SBUX: 31.5307\n",
      "  REGN: 18.9712\n",
      "  SNAP: 34.2578\n",
      "\n",
      "Downside_Volatility: 1 outliers\n",
      "  UNI-USD: 1.9946\n",
      "\n",
      "Rolling_Volatility: 1 outliers\n",
      "  UNI-USD: 1684.1292\n",
      "\n",
      "Win_Rate: 1 outliers\n",
      "  SHIB-USD: 0.1922\n",
      "\n",
      "Gain_Loss_Ratio: 2 outliers\n",
      "  SHIB-USD: 1.3625\n",
      "  UNI-USD: 754.9096\n",
      "\n",
      "Beta_SPY: 1 outliers\n",
      "  UNI-USD: 3741.7978\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ASSETS WITH MULTIPLE OUTLIER FEATURES\n",
      "--------------------------------------------------------------------------------\n",
      "  UNI-USD: 6 outlier features\n",
      "  SHIB-USD: 3 outlier features\n",
      "  NFLX: 2 outlier features\n",
      "  SBUX: 2 outlier features\n",
      "  PLTR: 1 outlier features\n",
      "  LCID: 1 outlier features\n",
      "  CT=F: 1 outlier features\n",
      "  XRP-USD: 1 outlier features\n",
      "  DOGE-USD: 1 outlier features\n",
      "  META: 1 outlier features\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDATION\n",
      "================================================================================\n",
      "âš  Severe outliers detected: UNI-USD has 6 extreme features\n",
      "âœ“ Will use RobustScaler (resistant to outliers)\n",
      "âœ“ Consider removing extreme outliers for cleaner clustering\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load features\n",
    "features = pd.read_csv(DATA_DIR / \"clustering_features.csv\", index_col=0)\n",
    "\n",
    "print(f\"\\nDataset shape: {features.shape}\")\n",
    "print(f\"Missing values: {features.isnull().sum().sum()}\")\n",
    "\n",
    "# Feature statistics\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"FEATURE STATISTICS\")\n",
    "print(\"-\" * 80)\n",
    "print(features.describe())\n",
    "\n",
    "# Detect outliers using IQR method\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"OUTLIER DETECTION (3Ã—IQR method)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "outlier_tickers = {}\n",
    "for col in features.columns:\n",
    "    Q1 = features[col].quantile(0.25)\n",
    "    Q3 = features[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 3 * IQR\n",
    "    upper = Q3 + 3 * IQR\n",
    "    \n",
    "    outliers = features[(features[col] < lower) | (features[col] > upper)]\n",
    "    if len(outliers) > 0:\n",
    "        print(f\"\\n{col}: {len(outliers)} outliers\")\n",
    "        for ticker in outliers.index[:5]:  # Show first 5\n",
    "            outlier_tickers[ticker] = outlier_tickers.get(ticker, 0) + 1\n",
    "            print(f\"  {ticker}: {outliers.loc[ticker, col]:.4f}\")\n",
    "\n",
    "# Assets with multiple outlier features\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"ASSETS WITH MULTIPLE OUTLIER FEATURES\")\n",
    "print(\"-\" * 80)\n",
    "sorted_outliers = sorted(outlier_tickers.items(), key=lambda x: x[1], reverse=True)\n",
    "for ticker, count in sorted_outliers[:10]:\n",
    "    print(f\"  {ticker}: {count} outlier features\")\n",
    "\n",
    "# Recommendation\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\" * 80)\n",
    "if len(sorted_outliers) > 0 and sorted_outliers[0][1] > 5:\n",
    "    print(f\"âš  Severe outliers detected: {sorted_outliers[0][0]} has {sorted_outliers[0][1]} extreme features\")\n",
    "    print(\"âœ“ Will use RobustScaler (resistant to outliers)\")\n",
    "    print(\"âœ“ Consider removing extreme outliers for cleaner clustering\")\n",
    "else:\n",
    "    print(\"âœ“ Moderate outliers present\")\n",
    "    print(\"âœ“ RobustScaler recommended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d68aec",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Remove extreme outliers and scale features for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1a6954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA PREPROCESSING\n",
      "================================================================================\n",
      "\n",
      "Original dataset: (225, 14)\n",
      "Removed extreme outliers: ['UNI-USD', 'SHIB-USD']\n",
      "Cleaned dataset: (223, 14)\n",
      "\n",
      "âœ“ Features scaled using RobustScaler\n",
      "\n",
      "Scaled feature statistics:\n",
      "       Mean_Return  Volatility  Sharpe_Ratio    Skewness    Kurtosis  \\\n",
      "count   223.000000  223.000000    223.000000  223.000000  223.000000   \n",
      "mean     -0.026298    0.294284     -0.012565    0.059680    0.517589   \n",
      "std       1.004994    0.973384      0.748923    1.428288    1.499868   \n",
      "min      -3.508829   -1.330390     -1.632102   -4.177939   -0.587403   \n",
      "25%      -0.473620   -0.360458     -0.500046   -0.524847   -0.271844   \n",
      "50%       0.000000    0.000000      0.000000    0.000000    0.000000   \n",
      "75%       0.526380    0.639542      0.499954    0.475153    0.728156   \n",
      "max       3.661246    3.040636      1.840693    8.430361    9.175861   \n",
      "\n",
      "       Downside_Volatility  Max_Drawdown  Percentile_5  Percentile_95  \\\n",
      "count           223.000000    223.000000    223.000000     223.000000   \n",
      "mean              0.255814     -0.132394     -0.263538       0.237247   \n",
      "std               0.900583      0.648746      1.046230       0.863988   \n",
      "min              -1.303912     -1.466785     -3.334443      -1.243687   \n",
      "25%              -0.357061     -0.628156     -0.561898      -0.373375   \n",
      "50%               0.000000      0.000000      0.000000       0.000000   \n",
      "75%               0.642939      0.371844      0.438102       0.626625   \n",
      "max               2.909097      1.145316      1.571665       2.815158   \n",
      "\n",
      "       Rolling_Volatility    Win_Rate  Gain_Loss_Ratio  Correlation_SPY  \\\n",
      "count          223.000000  223.000000       223.000000       223.000000   \n",
      "mean             0.282250   -0.012812         0.077058        -0.108001   \n",
      "std              0.997583    0.780709         0.776748         0.689224   \n",
      "min             -1.414656   -2.204082        -1.969468        -1.684875   \n",
      "25%             -0.393375   -0.489796        -0.435912        -0.567338   \n",
      "50%              0.000000    0.000000         0.000000         0.000000   \n",
      "75%              0.606625    0.510204         0.564088         0.432662   \n",
      "max              3.188904    2.163265         2.419726         1.285031   \n",
      "\n",
      "         Beta_SPY  \n",
      "count  223.000000  \n",
      "mean     0.015188  \n",
      "std      0.646533  \n",
      "min     -1.156317  \n",
      "25%     -0.477854  \n",
      "50%      0.000000  \n",
      "75%      0.522146  \n",
      "max      1.856941  \n",
      "\n",
      "âœ“ Ready for clustering:\n",
      "  223 assets\n",
      "  14 features\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load features\n",
    "features_raw = pd.read_csv(DATA_DIR / \"clustering_features.csv\", index_col=0)\n",
    "print(f\"\\nOriginal dataset: {features_raw.shape}\")\n",
    "\n",
    "# Remove extreme outliers (tickers with 5+ outlier features)\n",
    "outlier_counts = {}\n",
    "for col in features_raw.columns:\n",
    "    Q1 = features_raw[col].quantile(0.25)\n",
    "    Q3 = features_raw[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 3 * IQR\n",
    "    upper = Q3 + 3 * IQR\n",
    "    \n",
    "    outliers = features_raw[(features_raw[col] < lower) | (features_raw[col] > upper)]\n",
    "    for ticker in outliers.index:\n",
    "        outlier_counts[ticker] = outlier_counts.get(ticker, 0) + 1\n",
    "\n",
    "# Remove tickers with 5+ outlier features\n",
    "extreme_outliers = [ticker for ticker, count in outlier_counts.items() if count >= 5]\n",
    "features_clean = features_raw.drop(extreme_outliers)\n",
    "\n",
    "print(f\"Removed extreme outliers: {extreme_outliers}\")\n",
    "print(f\"Cleaned dataset: {features_clean.shape}\")\n",
    "\n",
    "# Scale features using RobustScaler (resistant to outliers)\n",
    "scaler = RobustScaler()\n",
    "features_scaled = scaler.fit_transform(features_clean)\n",
    "features_scaled_df = pd.DataFrame(\n",
    "    features_scaled, \n",
    "    index=features_clean.index, \n",
    "    columns=features_clean.columns\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Features scaled using RobustScaler\")\n",
    "print(f\"\\nScaled feature statistics:\")\n",
    "print(features_scaled_df.describe())\n",
    "\n",
    "# Store for clustering\n",
    "X = features_scaled\n",
    "tickers = features_clean.index.tolist()\n",
    "\n",
    "print(f\"\\nâœ“ Ready for clustering:\")\n",
    "print(f\"  {len(tickers)} assets\")\n",
    "print(f\"  {X.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92840173",
   "metadata": {},
   "source": [
    "# Clustering Model Training & Comparison\n",
    "\n",
    "Train multiple clustering algorithms and compare their performance for identifying shared risks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e59c1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CLUSTERING MODEL TRAINING (OPTIMIZED FOR MORE CLUSTERS)\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1. K-MEANS CLUSTERING\n",
      "--------------------------------------------------------------------------------\n",
      "k= 5 | Silhouette: 0.2230 | DB: 1.2596 | CH: 80.6\n",
      "k= 7 | Silhouette: 0.2099 | DB: 1.2812 | CH: 70.3\n",
      "k=10 | Silhouette: 0.1640 | DB: 1.2194 | CH: 61.0\n",
      "k=12 | Silhouette: 0.2017 | DB: 1.1940 | CH: 59.6\n",
      "k=15 | Silhouette: 0.1940 | DB: 1.1497 | CH: 55.4\n",
      "k=18 | Silhouette: 0.1864 | DB: 1.0596 | CH: 53.6\n",
      "k=20 | Silhouette: 0.2009 | DB: 1.0682 | CH: 50.6\n",
      "k=25 | Silhouette: 0.2106 | DB: 1.0718 | CH: 49.3\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2. HIERARCHICAL CLUSTERING\n",
      "--------------------------------------------------------------------------------\n",
      "k=10, ward     | Sil: 0.1926 | DB: 1.2470 | CH: 59.1\n",
      "k=10, complete | Sil: 0.1676 | DB: 1.0373 | CH: 48.4\n",
      "k=12, ward     | Sil: 0.1933 | DB: 1.1974 | CH: 56.1\n",
      "k=12, complete | Sil: 0.1759 | DB: 1.1447 | CH: 50.0\n",
      "k=15, ward     | Sil: 0.2110 | DB: 1.1189 | CH: 54.5\n",
      "k=15, complete | Sil: 0.2093 | DB: 1.0526 | CH: 51.2\n",
      "k=18, ward     | Sil: 0.1950 | DB: 1.1104 | CH: 52.4\n",
      "k=18, complete | Sil: 0.2065 | DB: 1.0721 | CH: 47.8\n",
      "k=20, ward     | Sil: 0.2063 | DB: 1.1198 | CH: 51.0\n",
      "k=20, complete | Sil: 0.2005 | DB: 1.0806 | CH: 45.8\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3. GAUSSIAN MIXTURE MODELS\n",
      "--------------------------------------------------------------------------------\n",
      "k=10 | Silhouette: 0.1782 | DB: 1.3259 | CH: 57.7\n",
      "k=12 | Silhouette: 0.1652 | DB: 1.3119 | CH: 53.0\n",
      "k=15 | Silhouette: 0.1940 | DB: 1.1497 | CH: 55.4\n",
      "k=18 | Silhouette: 0.2003 | DB: 1.1088 | CH: 52.9\n",
      "k=20 | Silhouette: 0.2021 | DB: 1.0731 | CH: 49.9\n",
      "\n",
      "âœ“ Trained 23 model configurations\n",
      "âœ“ Emphasis on 10-20 clusters for better granularity\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CLUSTERING MODEL TRAINING (OPTIMIZED FOR MORE CLUSTERS)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Storage for results\n",
    "results = []\n",
    "\n",
    "# ============================================================================\n",
    "# 1. K-MEANS CLUSTERING (test MORE clusters for granularity)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"1. K-MEANS CLUSTERING\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for k in [5, 7, 10, 12, 15, 18, 20, 25]:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    \n",
    "    sil_score = silhouette_score(X, labels)\n",
    "    db_score = davies_bouldin_score(X, labels)\n",
    "    ch_score = calinski_harabasz_score(X, labels)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': f'K-Means (k={k})',\n",
    "        'Algorithm': 'K-Means',\n",
    "        'N_Clusters': k,\n",
    "        'Silhouette': sil_score,\n",
    "        'Davies_Bouldin': db_score,\n",
    "        'Calinski_Harabasz': ch_score,\n",
    "        'Labels': labels\n",
    "    })\n",
    "    \n",
    "    print(f\"k={k:2d} | Silhouette: {sil_score:.4f} | DB: {db_score:.4f} | CH: {ch_score:.1f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. HIERARCHICAL CLUSTERING (more granular clusters)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"2. HIERARCHICAL CLUSTERING\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for k in [10, 12, 15, 18, 20]:\n",
    "    for linkage in ['ward', 'complete']:\n",
    "        hier = AgglomerativeClustering(n_clusters=k, linkage=linkage)\n",
    "        labels = hier.fit_predict(X)\n",
    "        \n",
    "        sil_score = silhouette_score(X, labels)\n",
    "        db_score = davies_bouldin_score(X, labels)\n",
    "        ch_score = calinski_harabasz_score(X, labels)\n",
    "        \n",
    "        results.append({\n",
    "            'Model': f'Hierarchical-{linkage} (k={k})',\n",
    "            'Algorithm': 'Hierarchical',\n",
    "            'N_Clusters': k,\n",
    "            'Silhouette': sil_score,\n",
    "            'Davies_Bouldin': db_score,\n",
    "            'Calinski_Harabasz': ch_score,\n",
    "            'Labels': labels\n",
    "        })\n",
    "        \n",
    "        print(f\"k={k:2d}, {linkage:8s} | Sil: {sil_score:.4f} | DB: {db_score:.4f} | CH: {ch_score:.1f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. GAUSSIAN MIXTURE MODELS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"3. GAUSSIAN MIXTURE MODELS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for k in [10, 12, 15, 18, 20]:\n",
    "    gmm = GaussianMixture(n_components=k, random_state=42)\n",
    "    labels = gmm.fit_predict(X)\n",
    "    \n",
    "    sil_score = silhouette_score(X, labels)\n",
    "    db_score = davies_bouldin_score(X, labels)\n",
    "    ch_score = calinski_harabasz_score(X, labels)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': f'GMM (k={k})',\n",
    "        'Algorithm': 'GMM',\n",
    "        'N_Clusters': k,\n",
    "        'Silhouette': sil_score,\n",
    "        'Davies_Bouldin': db_score,\n",
    "        'Calinski_Harabasz': ch_score,\n",
    "        'Labels': labels\n",
    "    })\n",
    "    \n",
    "    print(f\"k={k:2d} | Silhouette: {sil_score:.4f} | DB: {db_score:.4f} | CH: {ch_score:.1f}\")\n",
    "\n",
    "print(f\"\\nâœ“ Trained {len(results)} model configurations\")\n",
    "print(f\"âœ“ Emphasis on 10-20 clusters for better granularity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92216842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL COMPARISON\n",
      "================================================================================\n",
      "\n",
      "TOP 10 MODELS:\n",
      "                           Model  N_Clusters  Silhouette  Davies_Bouldin  \\\n",
      "0                  K-Means (k=5)           5    0.223037        1.259597   \n",
      "13  Hierarchical-complete (k=15)          15    0.209282        1.052636   \n",
      "12      Hierarchical-ward (k=15)          15    0.211026        1.118865   \n",
      "1                  K-Means (k=7)           7    0.209894        1.281177   \n",
      "7                 K-Means (k=25)          25    0.210613        1.071776   \n",
      "3                 K-Means (k=12)          12    0.201689        1.193965   \n",
      "6                 K-Means (k=20)          20    0.200950        1.068167   \n",
      "5                 K-Means (k=18)          18    0.186369        1.059597   \n",
      "15  Hierarchical-complete (k=18)          18    0.206527        1.072101   \n",
      "22                    GMM (k=20)          20    0.202071        1.073069   \n",
      "\n",
      "    Calinski_Harabasz   Avg_Rank  \n",
      "0           80.555422   7.333333  \n",
      "13          51.218940   7.333333  \n",
      "12          54.490292   7.666667  \n",
      "1           70.295587   9.000000  \n",
      "7           49.344094   9.333333  \n",
      "3           59.563062   9.666667  \n",
      "6           50.592540  10.333333  \n",
      "5           53.573006  10.666667  \n",
      "15          47.752175  11.333333  \n",
      "22          49.887252  11.333333  \n",
      "\n",
      "================================================================================\n",
      "BEST MODEL: K-Means (k=5)\n",
      "================================================================================\n",
      "Number of clusters: 5\n",
      "Silhouette Score: 0.2230\n",
      "Davies-Bouldin Score: 1.2596\n",
      "Calinski-Harabasz Score: 80.6\n",
      "\n",
      "CLUSTER SIZES:\n",
      "  Cluster 0: 4 assets\n",
      "  Cluster 1: 63 assets\n",
      "  Cluster 2: 13 assets\n",
      "  Cluster 3: 33 assets\n",
      "  Cluster 4: 110 assets\n",
      "\n",
      "âœ“ Saved clustering results to: data\\clustering_results.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame(results)\n",
    "comparison_df = comparison_df[['Model', 'Algorithm', 'N_Clusters', 'Silhouette', 'Davies_Bouldin', 'Calinski_Harabasz']]\n",
    "\n",
    "# Rank models by each metric\n",
    "# Silhouette: higher is better\n",
    "# Davies-Bouldin: lower is better\n",
    "# Calinski-Harabasz: higher is better\n",
    "\n",
    "comparison_df['Silhouette_Rank'] = comparison_df['Silhouette'].rank(ascending=False)\n",
    "comparison_df['DB_Rank'] = comparison_df['Davies_Bouldin'].rank(ascending=True)\n",
    "comparison_df['CH_Rank'] = comparison_df['Calinski_Harabasz'].rank(ascending=False)\n",
    "\n",
    "# Overall rank (average of ranks)\n",
    "comparison_df['Avg_Rank'] = (comparison_df['Silhouette_Rank'] + \n",
    "                              comparison_df['DB_Rank'] + \n",
    "                              comparison_df['CH_Rank']) / 3\n",
    "\n",
    "# Sort by average rank\n",
    "comparison_df = comparison_df.sort_values('Avg_Rank')\n",
    "\n",
    "print(\"\\nTOP 10 MODELS:\")\n",
    "print(comparison_df.head(10)[['Model', 'N_Clusters', 'Silhouette', 'Davies_Bouldin', 'Calinski_Harabasz', 'Avg_Rank']])\n",
    "\n",
    "# Select best model\n",
    "best_idx = comparison_df.iloc[0].name\n",
    "best_model_name = results[best_idx]['Model']\n",
    "best_labels = results[best_idx]['Labels']\n",
    "best_n_clusters = results[best_idx]['N_Clusters']\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"BEST MODEL: {best_model_name}\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"Number of clusters: {best_n_clusters}\")\n",
    "print(f\"Silhouette Score: {results[best_idx]['Silhouette']:.4f}\")\n",
    "print(f\"Davies-Bouldin Score: {results[best_idx]['Davies_Bouldin']:.4f}\")\n",
    "print(f\"Calinski-Harabasz Score: {results[best_idx]['Calinski_Harabasz']:.1f}\")\n",
    "\n",
    "# Add cluster labels to feature dataframe\n",
    "features_clean['Cluster'] = best_labels\n",
    "\n",
    "# Analyze cluster sizes\n",
    "print(f\"\\nCLUSTER SIZES:\")\n",
    "cluster_sizes = pd.Series(best_labels).value_counts().sort_index()\n",
    "for cluster_id, size in cluster_sizes.items():\n",
    "    print(f\"  Cluster {cluster_id}: {size} assets\")\n",
    "\n",
    "# Save results\n",
    "features_clean.to_csv(DATA_DIR / \"clustering_results.csv\")\n",
    "print(f\"\\nâœ“ Saved clustering results to: {DATA_DIR / 'clustering_results.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c7e8c6",
   "metadata": {},
   "source": [
    "# Cluster Analysis - Identifying Shared Risks\n",
    "\n",
    "Analyze each cluster to understand what risk profiles they represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e10b1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CLUSTER RISK PROFILE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "CLUSTER 0 - 4 assets\n",
      "================================================================================\n",
      "\n",
      "Asset Classes:\n",
      "  Crypto: 3 (75.0%)\n",
      "  Stock: 1 (25.0%)\n",
      "\n",
      "Risk Profile (median values):\n",
      "  Mean_Return         :     0.3166\n",
      "  Volatility          :     0.8999\n",
      "  Sharpe_Ratio        :     0.3341\n",
      "  Max_Drawdown        :    -0.7692\n",
      "  Win_Rate            :     0.4964\n",
      "  Correlation_SPY     :     0.2959\n",
      "  Beta_SPY            :     1.4296\n",
      "\n",
      "Representative Assets (closest to cluster center):\n",
      "  XLM-USD    (Crypto, Stellar)\n",
      "  SBUX       (Stock, Consumer)\n",
      "  DOGE-USD   (Crypto, Dogecoin)\n",
      "  XRP-USD    (Crypto, Ripple)\n",
      "\n",
      "Sample Assets:\n",
      "  XRP-USD, XLM-USD, SBUX, DOGE-USD\n",
      "\n",
      "================================================================================\n",
      "CLUSTER 1 - 63 assets\n",
      "================================================================================\n",
      "\n",
      "Asset Classes:\n",
      "  Stock: 39 (61.9%)\n",
      "  ETF: 16 (25.4%)\n",
      "  Commodity: 6 (9.5%)\n",
      "  Crypto: 2 (3.2%)\n",
      "\n",
      "Risk Profile (median values):\n",
      "  Mean_Return         :     0.0737\n",
      "  Volatility          :     0.4008\n",
      "  Sharpe_Ratio        :     0.1598\n",
      "  Max_Drawdown        :    -0.5573\n",
      "  Win_Rate            :     0.5006\n",
      "  Correlation_SPY     :     0.5978\n",
      "  Beta_SPY            :     1.4286\n",
      "\n",
      "Representative Assets (closest to cluster center):\n",
      "  USB        (Stock, Financial)\n",
      "  MU         (Stock, Tech)\n",
      "  ABNB       (Stock, Travel)\n",
      "  ASML       (Stock, Tech)\n",
      "  DVN        (Stock, Energy)\n",
      "\n",
      "Sample Assets:\n",
      "  BTC-USD, KC=F, AMZN, ARKW, CRM, KKR, ASML, MU, F, WEAT\n",
      "\n",
      "================================================================================\n",
      "CLUSTER 2 - 13 assets\n",
      "================================================================================\n",
      "\n",
      "Asset Classes:\n",
      "  Stock: 8 (61.5%)\n",
      "  Commodity: 5 (38.5%)\n",
      "\n",
      "Risk Profile (median values):\n",
      "  Mean_Return         :    -0.0228\n",
      "  Volatility          :     0.4396\n",
      "  Sharpe_Ratio        :    -0.0835\n",
      "  Max_Drawdown        :    -0.6074\n",
      "  Win_Rate            :     0.4994\n",
      "  Correlation_SPY     :     0.5285\n",
      "  Beta_SPY            :     1.3693\n",
      "\n",
      "Representative Assets (closest to cluster center):\n",
      "  ZC=F       (Commodity, Corn)\n",
      "  META       (Stock, Tech)\n",
      "  DOCU       (Stock, Tech)\n",
      "  CT=F       (Commodity, Cotton)\n",
      "  CSCO       (Stock, Tech)\n",
      "\n",
      "Sample Assets:\n",
      "  CC=F, ZC=F, META, HO=F, PYPL, NFLX, ADBE, HE=F, INTC, DOCU\n",
      "\n",
      "================================================================================\n",
      "CLUSTER 3 - 33 assets\n",
      "================================================================================\n",
      "\n",
      "Asset Classes:\n",
      "  Stock: 18 (54.5%)\n",
      "  Crypto: 13 (39.4%)\n",
      "  Commodity: 2 (6.1%)\n",
      "\n",
      "Risk Profile (median values):\n",
      "  Mean_Return         :    -0.0967\n",
      "  Volatility          :     0.8192\n",
      "  Sharpe_Ratio        :    -0.1230\n",
      "  Max_Drawdown        :    -0.8985\n",
      "  Win_Rate            :     0.4828\n",
      "  Correlation_SPY     :     0.4166\n",
      "  Beta_SPY            :     1.9277\n",
      "\n",
      "Representative Assets (closest to cluster center):\n",
      "  MATIC-USD  (Crypto, Polygon)\n",
      "  LINK-USD   (Crypto, Chainlink)\n",
      "  ETH-USD    (Crypto, Ethereum)\n",
      "  DDOG       (Stock, Tech)\n",
      "  VET-USD    (Crypto, VeChain)\n",
      "\n",
      "Sample Assets:\n",
      "  ICP-USD, RIVN, AVAX-USD, NIO, MDB, TEAM, UNG, ADA-USD, W, MRNA\n",
      "\n",
      "================================================================================\n",
      "CLUSTER 4 - 111 assets\n",
      "================================================================================\n",
      "\n",
      "Asset Classes:\n",
      "  Stock: 50 (45.0%)\n",
      "  ETF: 44 (39.6%)\n",
      "  Commodity: 17 (15.3%)\n",
      "\n",
      "Risk Profile (median values):\n",
      "  Mean_Return         :     0.0951\n",
      "  Volatility          :     0.2421\n",
      "  Sharpe_Ratio        :     0.4403\n",
      "  Max_Drawdown        :    -0.3209\n",
      "  Win_Rate            :     0.5208\n",
      "  Correlation_SPY     :     0.6080\n",
      "  Beta_SPY            :     0.7257\n",
      "\n",
      "Representative Assets (closest to cluster center):\n",
      "  ICE        (Stock, Financial)\n",
      "  HD         (Stock, Retail)\n",
      "  CVX        (Stock, Energy)\n",
      "  MDLZ       (Stock, Consumer)\n",
      "  MS         (Stock, Financial)\n",
      "\n",
      "Sample Assets:\n",
      "  VDE, UNH, V, VCR, VNQ, XLF, MPC, SPGI, PL=F, HD\n",
      "\n",
      "================================================================================\n",
      "CLUSTER INTERPRETATION\n",
      "================================================================================\n",
      "Clusters represent assets with similar risk profiles:\n",
      "  - Similar volatility patterns\n",
      "  - Similar return characteristics\n",
      "  - Similar market correlations\n",
      "  - Similar drawdown behaviors\n",
      "\n",
      "Assets in the same cluster share risks and tend to move together.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CLUSTER RISK PROFILE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load asset metadata\n",
    "summary = pd.read_csv(DATA_DIR / \"asset_summary.csv\")\n",
    "results_df = features_clean.reset_index().rename(columns={'index': 'Ticker'})\n",
    "results_df = results_df.merge(summary[['Ticker', 'Asset_Class', 'Category']], on='Ticker', how='left')\n",
    "\n",
    "# Analyze each cluster\n",
    "for cluster_id in sorted(results_df['Cluster'].unique()):\n",
    "    cluster_data = results_df[results_df['Cluster'] == cluster_id]\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"CLUSTER {cluster_id} - {len(cluster_data)} assets\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    # Asset class distribution\n",
    "    print(f\"\\nAsset Classes:\")\n",
    "    asset_class_dist = cluster_data['Asset_Class'].value_counts()\n",
    "    for ac, count in asset_class_dist.items():\n",
    "        pct = count / len(cluster_data) * 100\n",
    "        print(f\"  {ac}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Key statistics (using raw features)\n",
    "    feature_cols = ['Mean_Return', 'Volatility', 'Sharpe_Ratio', 'Max_Drawdown', \n",
    "                    'Win_Rate', 'Correlation_SPY', 'Beta_SPY']\n",
    "    \n",
    "    print(f\"\\nRisk Profile (median values):\")\n",
    "    for col in feature_cols:\n",
    "        median_val = cluster_data[col].median()\n",
    "        print(f\"  {col:20s}: {median_val:10.4f}\")\n",
    "    \n",
    "    # Representative assets (closest to cluster center)\n",
    "    cluster_center = cluster_data[features_clean.columns[:-1]].mean()\n",
    "    cluster_data['Distance_to_Center'] = cluster_data[features_clean.columns[:-1]].apply(\n",
    "        lambda row: np.sqrt(((row - cluster_center) ** 2).sum()), axis=1\n",
    "    )\n",
    "    top_reps = cluster_data.nsmallest(5, 'Distance_to_Center')\n",
    "    \n",
    "    print(f\"\\nRepresentative Assets (closest to cluster center):\")\n",
    "    for idx, row in top_reps.iterrows():\n",
    "        print(f\"  {row['Ticker']:10s} ({row['Asset_Class']}, {row['Category']})\")\n",
    "    \n",
    "    # Sample of assets in cluster\n",
    "    print(f\"\\nSample Assets:\")\n",
    "    sample = cluster_data.sample(min(10, len(cluster_data)), random_state=42)\n",
    "    print(f\"  {', '.join(sample['Ticker'].tolist())}\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"CLUSTER INTERPRETATION\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(\"Clusters represent assets with similar risk profiles:\")\n",
    "print(\"  - Similar volatility patterns\")\n",
    "print(\"  - Similar return characteristics\")\n",
    "print(\"  - Similar market correlations\")\n",
    "print(\"  - Similar drawdown behaviors\")\n",
    "print(\"\\nAssets in the same cluster share risks and tend to move together.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8308ce",
   "metadata": {},
   "source": [
    "# Intelligent Cluster Naming\n",
    "\n",
    "Automatically name each cluster based on its risk profile and composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45345759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INTELLIGENT CLUSTER NAMING\n",
      "================================================================================\n",
      "\n",
      "Cluster  0 â†’ 'High-Drawdown Market-Independent Cryptocurrencies'\n",
      "  4 assets | Vol: 0.90 | Ret: 0.32 | Sharpe: 0.33 | Beta: 1.43\n",
      "  Top asset classes: Crypto(3), Stock(1)\n",
      "\n",
      "Cluster  1 â†’ 'High-Beta Digital Assets'\n",
      "  63 assets | Vol: 0.40 | Ret: 0.07 | Sharpe: 0.16 | Beta: 1.43\n",
      "  Top asset classes: Stock(39), ETF(16), Commodity(6)\n",
      "\n",
      "Cluster  2 â†’ 'High-Beta Commodities'\n",
      "  13 assets | Vol: 0.44 | Ret: -0.02 | Sharpe: -0.08 | Beta: 1.37\n",
      "  Top asset classes: Stock(8), Commodity(5)\n",
      "\n",
      "Cluster  3 â†’ 'High-Drawdown High-Beta Commodities'\n",
      "  33 assets | Vol: 0.82 | Ret: -0.10 | Sharpe: -0.12 | Beta: 1.93\n",
      "  Top asset classes: Stock(18), Crypto(13), Commodity(2)\n",
      "\n",
      "Cluster  4 â†’ 'Low-Risk Diversified'\n",
      "  111 assets | Vol: 0.24 | Ret: 0.10 | Sharpe: 0.44 | Beta: 0.73\n",
      "  Top asset classes: Stock(50), ETF(44), Commodity(17)\n",
      "\n",
      "âœ“ Saved named clusters to: data\\clustering_results_named.csv\n",
      "\n",
      "================================================================================\n",
      "CLUSTER SUMMARY\n",
      "================================================================================\n",
      " Cluster_ID                                      Cluster_Name  Size  Volatility    Return    Sharpe     Beta             Top_Assets\n",
      "          0 High-Drawdown Market-Independent Cryptocurrencies     4    0.899929  0.316556  0.334107 1.429599 SBUX, XLM-USD, XRP-USD\n",
      "          1                          High-Beta Digital Assets    63    0.400751  0.073691  0.159819 1.428573         BOTZ, UFO, BAC\n",
      "          2                             High-Beta Commodities    13    0.439568 -0.022810 -0.083528 1.369332       CSCO, ZC=F, CT=F\n",
      "          3               High-Drawdown High-Beta Commodities    33    0.819230 -0.096658 -0.122999 1.927726       ETSY, DDOG, TEAM\n",
      "          4                              Low-Risk Diversified   111    0.242102  0.095064  0.440293 0.725746          SHY, AGG, HYG\n"
     ]
    }
   ],
   "source": [
    "def name_cluster(cluster_data, cluster_id):\n",
    "    \"\"\"\n",
    "    Intelligently name a cluster based on its characteristics\n",
    "    \"\"\"\n",
    "    # Extract key metrics\n",
    "    median_vol = cluster_data['Volatility'].median()\n",
    "    median_return = cluster_data['Mean_Return'].median()\n",
    "    median_sharpe = cluster_data['Sharpe_Ratio'].median()\n",
    "    median_beta = cluster_data['Beta_SPY'].median()\n",
    "    median_corr = cluster_data['Correlation_SPY'].median()\n",
    "    median_drawdown = cluster_data['Max_Drawdown'].median()\n",
    "    \n",
    "    # Asset class composition\n",
    "    asset_classes = cluster_data['Asset_Class'].value_counts()\n",
    "    dominant_class = asset_classes.index[0] if len(asset_classes) > 0 else \"Mixed\"\n",
    "    dominant_pct = asset_classes.iloc[0] / len(cluster_data) * 100\n",
    "    \n",
    "    # Category composition\n",
    "    categories = cluster_data['Category'].value_counts()\n",
    "    dominant_category = categories.index[0] if len(categories) > 0 else \"Mixed\"\n",
    "    \n",
    "    # Determine cluster name based on characteristics\n",
    "    name_parts = []\n",
    "    \n",
    "    # 1. Risk level (based on volatility)\n",
    "    if median_vol < 0.3:\n",
    "        risk_level = \"Low-Risk\"\n",
    "    elif median_vol < 0.6:\n",
    "        risk_level = \"Moderate-Risk\"\n",
    "    elif median_vol < 1.0:\n",
    "        risk_level = \"High-Risk\"\n",
    "    else:\n",
    "        risk_level = \"Very High-Risk\"\n",
    "    \n",
    "    # 2. Asset type or strategy\n",
    "    if dominant_pct > 70:\n",
    "        if dominant_class == \"Stock\":\n",
    "            if dominant_category in [\"Tech\", \"Healthcare\", \"Financial\", \"Energy\", \"Consumer\"]:\n",
    "                asset_type = f\"{dominant_category} Stocks\"\n",
    "            else:\n",
    "                asset_type = \"Equities\"\n",
    "        elif dominant_class == \"ETF\":\n",
    "            asset_type = \"ETFs\"\n",
    "        elif dominant_class == \"Commodity\":\n",
    "            if \"Gold\" in dominant_category or \"Silver\" in dominant_category:\n",
    "                asset_type = \"Precious Metals\"\n",
    "            else:\n",
    "                asset_type = \"Commodities\"\n",
    "        elif dominant_class == \"Crypto\":\n",
    "            asset_type = \"Cryptocurrencies\"\n",
    "        else:\n",
    "            asset_type = dominant_class\n",
    "    else:\n",
    "        # Mixed composition\n",
    "        if \"Commodity\" in asset_classes.index and \"ETF\" not in asset_classes.index[:2]:\n",
    "            asset_type = \"Commodities\"\n",
    "        elif \"Crypto\" in asset_classes.index:\n",
    "            asset_type = \"Digital Assets\"\n",
    "        else:\n",
    "            asset_type = \"Diversified\"\n",
    "    \n",
    "    # 3. Market behavior\n",
    "    if median_corr > 0.7 and median_beta > 0.9 and median_beta < 1.1:\n",
    "        behavior = \"Market-Following\"\n",
    "    elif median_corr < 0.3:\n",
    "        behavior = \"Market-Independent\"\n",
    "    elif median_beta > 1.3:\n",
    "        behavior = \"High-Beta\"\n",
    "    elif median_beta < 0.7:\n",
    "        behavior = \"Defensive\"\n",
    "    else:\n",
    "        behavior = None\n",
    "    \n",
    "    # 4. Special characteristics\n",
    "    if median_return > 0.5 and median_sharpe > 0.5:\n",
    "        special = \"Growth\"\n",
    "    elif median_return < -0.1:\n",
    "        special = \"Declining\"\n",
    "    elif abs(median_drawdown) > 0.7:\n",
    "        special = \"High-Drawdown\"\n",
    "    elif \"Gold\" in dominant_category or \"Silver\" in dominant_category or dominant_class == \"Commodity\":\n",
    "        special = \"Store of Value\"\n",
    "    else:\n",
    "        special = None\n",
    "    \n",
    "    # Construct final name\n",
    "    if special:\n",
    "        name_parts.append(special)\n",
    "    if behavior:\n",
    "        name_parts.append(behavior)\n",
    "    name_parts.append(asset_type)\n",
    "    \n",
    "    # Create clean name\n",
    "    if len(name_parts) == 1:\n",
    "        cluster_name = f\"{risk_level} {name_parts[0]}\"\n",
    "    else:\n",
    "        cluster_name = \" \".join(name_parts)\n",
    "    \n",
    "    return cluster_name\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"INTELLIGENT CLUSTER NAMING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create cluster names\n",
    "cluster_names = {}\n",
    "cluster_descriptions = {}\n",
    "\n",
    "for cluster_id in sorted(results_df['Cluster'].unique()):\n",
    "    cluster_data = results_df[results_df['Cluster'] == cluster_id]\n",
    "    \n",
    "    # Generate name\n",
    "    name = name_cluster(cluster_data, cluster_id)\n",
    "    cluster_names[cluster_id] = name\n",
    "    \n",
    "    # Generate description\n",
    "    median_vol = cluster_data['Volatility'].median()\n",
    "    median_return = cluster_data['Mean_Return'].median()\n",
    "    median_sharpe = cluster_data['Sharpe_Ratio'].median()\n",
    "    median_beta = cluster_data['Beta_SPY'].median()\n",
    "    asset_classes = cluster_data['Asset_Class'].value_counts()\n",
    "    \n",
    "    desc = f\"{len(cluster_data)} assets | Vol: {median_vol:.2f} | Ret: {median_return:.2f} | Sharpe: {median_sharpe:.2f} | Beta: {median_beta:.2f}\"\n",
    "    cluster_descriptions[cluster_id] = desc\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_id:2d} â†’ '{name}'\")\n",
    "    print(f\"  {desc}\")\n",
    "    print(f\"  Top asset classes: {', '.join([f'{k}({v})' for k,v in asset_classes.head(3).items()])}\")\n",
    "\n",
    "# Add names to results dataframe\n",
    "results_df['Cluster_Name'] = results_df['Cluster'].map(cluster_names)\n",
    "\n",
    "# Save updated results\n",
    "results_df.to_csv(DATA_DIR / \"clustering_results_named.csv\", index=False)\n",
    "print(f\"\\nâœ“ Saved named clusters to: {DATA_DIR / 'clustering_results_named.csv'}\")\n",
    "\n",
    "# Create summary table\n",
    "cluster_summary = []\n",
    "for cluster_id in sorted(cluster_names.keys()):\n",
    "    cluster_data = results_df[results_df['Cluster'] == cluster_id]\n",
    "    cluster_summary.append({\n",
    "        'Cluster_ID': cluster_id,\n",
    "        'Cluster_Name': cluster_names[cluster_id],\n",
    "        'Size': len(cluster_data),\n",
    "        'Volatility': cluster_data['Volatility'].median(),\n",
    "        'Return': cluster_data['Mean_Return'].median(),\n",
    "        'Sharpe': cluster_data['Sharpe_Ratio'].median(),\n",
    "        'Beta': cluster_data['Beta_SPY'].median(),\n",
    "        'Top_Assets': ', '.join(cluster_data.nsmallest(3, 'Volatility')['Ticker'].tolist())\n",
    "    })\n",
    "\n",
    "cluster_summary_df = pd.DataFrame(cluster_summary)\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"CLUSTER SUMMARY\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(cluster_summary_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
